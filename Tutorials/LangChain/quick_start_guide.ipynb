{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __[Quickstart Guide](https://python.langchain.com/docs/get_started/quickstart)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Building A Language Model Application__\n",
    "\n",
    "__LLMS: Get predictions from a language model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize our model...\n",
    "llm = OpenAI(temperature=0.9) \n",
    "# temperature is the amount of randomness in your model \n",
    "# temperature = 0 is a deterministic model --> same result over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can querey our model within OpenAI API\n",
    "text = \"What is a vacation destination for someone who likes to eat pasta?\"\n",
    "print(llm(text))\n",
    "\n",
    "# Returns:\n",
    "# 1. Rome, Italy\n",
    "# 2. Bologna, Italy\n",
    "# 3. Venice, Italy\n",
    "# 4. Amalfi Coast, Italy\n",
    "# 5. Sicily, Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prompt Templates: Manage prompts for LLMS__\n",
    "\n",
    "* Makes it easier to maneuver and scale prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    imput_variables=[\"food\"], \n",
    "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\" # using `food` variable that we can create a template around\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.format(food=\"dessert\"))\n",
    "\n",
    "# Returns:\n",
    "# What are 5 vacation destinations for someone who likes to eat dessert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's feed our prompt into the large language model (LLM)...\n",
    "print(llm(prompt.format(food=\"dessert\")))\n",
    "\n",
    "# Returns:\n",
    "# 1. New York City, USA\n",
    "# 2. Tokyo, Japan\n",
    "# 3. Paris, France\n",
    "# 4. San Francisco, USA\n",
    "# 5. Vancouver, Canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chains: Combine LLMs and prompts in multi-step workflows__\n",
    "\n",
    "* what if your promp needs multiple questions and multiple answers in order to complete them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['food'],\n",
    "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"fruit\"))\n",
    "\n",
    "# Returns:\n",
    "# 1. Costa Rica\n",
    "# 2. Hawaii\n",
    "# 3. Malaysia\n",
    "# 4. India\n",
    "# 5. Thailand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Agents: Dynamically call chains based on user input__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some tools to use\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = '...'ArithmeticError\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's initialize an agent with:\n",
    "# 1. The tools\n",
    "# 2. The language model\n",
    "# 3. The type of agent we want to use\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-discription\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See list of agent types [here](https://python.langchain.com/docs/modules/agents/agent_types/#:~:text=Zero%2Dshot%20ReAct%E2%80%8B,is%20provided%20for%20each%20tool.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test it out!\n",
    "agent.run(\"Who is the current leader of Japan? What is the largets prime number is smaller than their age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell returns:\n",
    "\n",
    "<img src=\"../images/agents.png\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Memory: Add state to chains and agents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm), verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell returns:\n",
    "\n",
    "<img src=\"../images/memory.png\" height=130>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell runs:\n",
    "\n",
    "<img src=\"../images/memory_2.png\" height=150>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell returns:\n",
    "\n",
    "<img src=\"../images/memory_3.png\" height=170>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is an alternative phrase for the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell returns:\n",
    "\n",
    "<img src=\"../images/memory_4.png\" height=180>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
